# Automated Data Quality Pipeline 

## Business Context
In industrial and retail environments, raw data often contains formatting errors, duplicates, and missing values. Manual cleaning is slow and error-prone. 
This project automates the cleaning process to ensure downstream AI models receive high-quality data. 

**I built this to solve the "Garbage In, Garbage Out" problem in AI pipelines.**

## ✦ Solution Architecture
I built a Python ETL pipeline that:
1.  **Ingests** raw CSV data (simulating dirty real-world input).
2.  **Validates** schema integrity (removing duplicates, standardizing snake_case).
3.  **Imputes** missing values based on business logic (e.g., handling missing Promotions).
4.  **Generates an Audit Log** automatically tracking every change made to the data.

## ✦ Key Features
- **Audit Trail:** Automatically generates a `logs/audit_logs.txt` file (see example below).
- **Regex Cleaning:** Standardizes text fields using Regular Expressions.
- **Scalable:** Built with Pandas to handle large datasets efficiently.

## ✦ Project Structure
```text
├── data/               # Raw and Processed data (Ignored by Git)
├── logs/               # Audit logs generated by the script
├── notebooks/          # Jupyter Notebooks for EDA and prototyping
├── src/                # Production-ready Python scripts
└── requirements.txt    # Dependencies
```
## ✦ Example Audit Log Output
```text
[2025-12-23 18:35:03] PIPELINE STARTED. Loaded 1,000,000 rows.
[2025-12-23 18:35:04] Standardized column names to snake_case.
[2025-12-23 18:35:04] Imputed 333,943 missing 'promotion' values with 'None'.
[2025-12-23 18:35:04] Converted 'date' column to datetime objects.
[2025-12-23 18:35:08] SUCCESS: Saved clean data to ../data/processed/clean_data.csv
