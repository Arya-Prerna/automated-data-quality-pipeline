# Automated Data Quality Pipeline ðŸ› ï¸

## Business Context
In industrial and retail environments, raw data often contains formatting errors, duplicates, and missing values. Manual cleaning is slow and error-prone. 
This project automates the cleaning process to ensure downstream AI models receive high-quality data. 

**I built this to solve the "Garbage In, Garbage Out" problem in AI pipelines.**

## -> Solution Architecture
I built a Python ETL pipeline that:
1.  **Ingests** raw CSV data (simulating dirty real-world input).
2.  **Validates** schema integrity (removing duplicates, standardizing snake_case).
3.  **Imputes** missing values based on business logic (e.g., handling missing Promotions).
4.  **Generates an Audit Log** automatically tracking every change made to the data.

## -> Key Features
- **Audit Trail:** Automatically generates a `logs/audit_logs.txt` file (see example below).
- **Regex Cleaning:** Standardizes text fields using Regular Expressions.
- **Scalable:** Built with Pandas to handle large datasets efficiently.

## ðŸ“‚ Project Structure
```text
â”œâ”€â”€ data/               # Raw and Processed data (Ignored by Git)
â”œâ”€â”€ logs/               # Audit logs generated by the script
â”œâ”€â”€ notebooks/          # Jupyter Notebooks for EDA and prototyping
â”œâ”€â”€ src/                # Production-ready Python scripts
â””â”€â”€ requirements.txt    # Dependencies
